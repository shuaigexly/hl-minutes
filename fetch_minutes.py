# fetch_minutes.py

import os
import json
import time
from datetime import datetime
from typing import Dict, Any, List

import pandas as pd
from hyperliquid.info import Info

from config import COINS, INTERVALS, DATA_DIR, CHECKPOINT_FILE, CHUNK_HOURS, API_SLEEP

# é¦–æ¬¡å…¨é‡æ—¶å‘å‰è¿½æº¯çš„å¹´æ•°
YEARS_BACK = 3


def get_info_client() -> Info:
    """
    åˆ›å»º Hyperliquid Info å®¢æˆ·ç«¯ï¼ˆä¸éœ€è¦ websocketï¼‰
    """
    return Info(skip_ws=True)


def load_checkpoint() -> Dict[str, int]:
    """
    åŠ è½½æ‰€æœ‰ coin_interval çš„æ–­ç‚¹ï¼Œ
    æ ¼å¼å¤§æ¦‚æ˜¯ï¼š
    {
        "BTC_1m": 1731800000000,
        "ETH_5m": 1731700000000,
        ...
    }
    """
    if not os.path.exists(CHECKPOINT_FILE):
        return {}
    with open(CHECKPOINT_FILE, "r") as f:
        return json.load(f)


def save_checkpoint(ck: Dict[str, int]) -> None:
    """
    æŠŠæ‰€æœ‰æ–­ç‚¹å†™å› checkpoint.json
    """
    with open(CHECKPOINT_FILE, "w") as f:
        json.dump(ck, f)


def ensure_dirs() -> None:
    """
    åˆ›å»ºæ•°æ®ç›®å½•ï¼š
    data/BTC/1m.parquet
    data/ETH/5m.parquet
    ...
    """
    for coin in COINS:
        coin_dir = os.path.join(DATA_DIR, coin)
        os.makedirs(coin_dir, exist_ok=True)


def parquet_path(coin: str, interval: str) -> str:
    return os.path.join(DATA_DIR, coin, f"{interval}.parquet")


def fetch_incremental_for_pair(
    info: Info,
    coin: str,
    interval: str,
    checkpoints: Dict[str, int],
) -> List[Dict[str, Any]]:
    """
    å¯¹å•ä¸ª (coin, interval) åšå¢é‡/å…¨é‡æ‹‰å–ï¼Œè¿”å›æ–°å¢çš„ rows åˆ—è¡¨ã€‚
    """
    key = f"{coin}_{interval}"
    now_ms = int(time.time() * 1000)

    # å†³å®šèµ·ç‚¹ï¼šæ˜¯æ–­ç‚¹ç»­ä¼ è¿˜æ˜¯ä» YEARS_BACK å¹´å‰å¼€å§‹
    if key in checkpoints:
        start_ms = checkpoints[key] + 1
        print(f"â¯ [{coin} {interval}] æ–­ç‚¹ç»­ä¼ ï¼Œä» {datetime.utcfromtimestamp(start_ms/1000)} å¼€å§‹æ‹‰å–")
    else:
        start_ms = now_ms - YEARS_BACK * 365 * 24 * 3600 * 1000
        print(f"ğŸ”° [{coin} {interval}] é¦–æ¬¡è¿è¡Œï¼Œä» {YEARS_BACK} å¹´å‰å¼€å§‹æ‹‰å–")

    if start_ms >= now_ms:
        print(f"â„¹ [{coin} {interval}] å·²ç»æ˜¯æœ€æ–°ï¼Œæ— éœ€æ‹‰å–")
        return []

    chunk_ms = CHUNK_HOURS * 3600 * 1000
    cursor = start_ms
    all_rows: List[Dict[str, Any]] = []

    while cursor < now_ms:
        chunk_start = cursor
        chunk_end = min(cursor + chunk_ms, now_ms)

        print(
            f"â± [{coin} {interval}] è·å–åŒºé—´ "
            f"{datetime.utcfromtimestamp(chunk_start/1000)} â†’ {datetime.utcfromtimestamp(chunk_end/1000)}"
        )

        try:
            data = info.candles_snapshot(
                name=coin,
                interval=interval,
                startTime=chunk_start,
                endTime=chunk_end,
            )
        except Exception as e:
            print(f"âŒ [{coin} {interval}] è¯·æ±‚å¤±è´¥: {e}ï¼Œ5 ç§’åé‡è¯•")
            time.sleep(5)
            continue

        if data:
            all_rows.extend(data)
            # ä»¥æœ€åä¸€æ ¹Kçº¿çš„å¼€ç›˜æ—¶é—´ t ä½œä¸ºæ–­ç‚¹
            latest_t = data[-1]["t"]
            checkpoints[key] = latest_t
            save_checkpoint(checkpoints)

        cursor = chunk_end + 1
        time.sleep(API_SLEEP)

    return all_rows


def rows_to_df(rows: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    æŠŠ SDK è¿”å›çš„ list[dict] è½¬æˆ DataFrameï¼Œå¹¶å¤„ç†æ—¶é—´å­—æ®µ
    """
    if not rows:
        return pd.DataFrame()

    df = pd.DataFrame(rows)
    # t / T æ˜¯æ¯«ç§’æ—¶é—´æˆ³
    if "t" in df.columns:
        df["t"] = pd.to_datetime(df["t"], unit="ms")
    if "T" in df.columns:
        df["T"] = pd.to_datetime(df["T"], unit="ms")
    return df


def save_parquet_incremental(coin: str, interval: str, df_new: pd.DataFrame) -> None:
    """
    æŠŠæ–°å¢æ•°æ® df_new åˆå¹¶åˆ° data/{coin}/{interval}.parquet ä¸­ã€‚
    ç”¨ t å»é‡ï¼Œä¿è¯ä¸ä¼šé‡å¤ã€‚
    """
    if df_new.empty:
        print(f"â„¹ [{coin} {interval}] æ²¡æœ‰æ–°æ•°æ®ï¼Œä¸å†™å…¥")
        return

    ensure_dirs()
    path = parquet_path(coin, interval)

    if os.path.exists(path):
        df_old = pd.read_parquet(path)
        df = pd.concat([df_old, df_new], axis=0)
        if "t" in df.columns:
            df = df.drop_duplicates(subset=["t"]).sort_values("t")
    else:
        df = df_new

    df.to_parquet(path, index=False)
    print(f"âœ… [{coin} {interval}] ä¿å­˜æˆåŠŸ: {path}  å…± {len(df)} è¡Œ")


def main():
    info = get_info_client()
    checkpoints = load_checkpoint()

    for coin in COINS:
        for interval in INTERVALS:
            print(f"\n=== å¼€å§‹å¤„ç† {coin} {interval} ===")
            rows = fetch_incremental_for_pair(info, coin, interval, checkpoints)
            df = rows_to_df(rows)
            save_parquet_incremental(coin, interval, df)

    print("\nğŸ‰ æ‰€æœ‰å¸ç§ & å‘¨æœŸå¤„ç†å®Œæˆ")


if __name__ == "__main__":
    main()
